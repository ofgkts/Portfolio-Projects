{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03.08.22 scrape(kariyernetscrape)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install requests-html\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "def extract(page):\n",
        "      headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'}\n",
        "      url=f'https://www.kariyer.net/is-ilanlari/afyon-{page}?ct=3&cp={page}'\n",
        "      r=requests.get(url,headers)\n",
        "      soup=BeautifulSoup(r.content,'html.parser')\n",
        "      return soup\n",
        "\n",
        "def transform(soup):\n",
        "  divs=soup.find_all(\"div\",class_='list-items')\n",
        "  for item in divs:\n",
        "    company_name=item.find(\"div\",class_='k-text small').text.strip()\n",
        "    position=item.find(\"h3\",class_=\"kad-card-title\").text.strip()\n",
        "    worktime=item.find('div',class_='k-space medium right').text.strip()\n",
        "    job={'company name':company_name,'position':position,\"worktime\":worktime }\n",
        "    joblist.append(job)\n",
        "  return\n",
        "\n",
        "\n",
        "joblist=[]\n",
        "for i in range(1,13):\n",
        "  c=extract(1)\n",
        "  transform(c)\n",
        "\n",
        "df=pd.DataFrame(joblist)\n",
        "df.to_csv(\"jobs.csv\")\n",
        "\n",
        "\n",
        "\n",
        "#yeeeey i did it.i think its waaay more better\n",
        "#if you have any feedbacks i am openmind \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KHv0Dsj61p1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5681e6d-bdc4-4d58-903e-065095bde725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        company name  \\\n",
            "0  LAV MEDYA YAYINCILIK REKLAM YAZILIM TASARIM OR...   \n",
            "1       Tommy Hilfiger Marka Dağıtım ve Ticaret A.Ş.   \n",
            "2  Yapı Merkezi İdis Mühendislik Sanayi ve Ticare...   \n",
            "3               KARACA ZÜCCACİYE TİCARET VE SAN A.Ş.   \n",
            "4                      AgeSA Hayat ve Emeklilik A.Ş.   \n",
            "\n",
            "                            position     worktime  \n",
            "0          Kurumsal Satış Temsilcisi  Tam zamanlı  \n",
            "1    Genel Part Time Satış Danışmanı    Part Time  \n",
            "2           Uzman Planlama Mühendisi  Tam zamanlı  \n",
            "3                Mağaza Müdürü Afium  Tam zamanlı  \n",
            "4  Home Office - Java Yazılım Uzmanı  Tam zamanlı  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqjo712ZWxDD"
      },
      "outputs": [],
      "source": [
        "#just chaos in here i am trying to figure it out how can i crate loops etc.\n",
        "\n",
        "#from bs4 import BeautifulSoup\n",
        "#from bs4 import BeautifulSoup\n",
        "#import requests\n",
        "#import pandas as pd\n",
        "#bs=BeautifulSoup()\n",
        "##first  i need to crate loop for url's multiple page \n",
        "#page = 1\n",
        "#titles = []\n",
        "#while page != 13:\n",
        "#      url = f\"https://www.kariyer.net/is-ilanlari/afyon-{page}?ct=3&cp={page}\"\n",
        "#      response = requests.get(url)\n",
        "#      html = response.content\n",
        "#      soup = bs(html, \"lxml\")\n",
        "#      for job in bs.find_all(\"div\",class_='list-items'):\n",
        "#        company_name=job.find(\"div\",class_='k-text small').text.replace(\"/n\",\"\")\n",
        "#        position=job.find(\"h3\",class_=\"kad-card-title\").text.replace(\"/n\",\"\")\n",
        "#        worktime=job.find('div',class_='k-space medium right').text.replace(\"/n\",\"_\")\n",
        "#        results=titles.append(jobs)    \n",
        "#      page = page + 1\n",
        "#      #its working but code is a bit fucked up so i need to recrate it\n",
        "#jobs=soup.find_all(\"div\",class_='list-items')\n",
        "#for job in jobs:\n",
        "#        company_name=job.find(\"div\",class_='k-text small').text.replace(\"/n\",\"\")\n",
        "#        position=job.find(\"h3\",class_=\"kad-card-title\").text.replace(\"/n\",\"\")\n",
        "#        worktime=job.find('div',class_='k-space medium right').text.replace(\"/n\",\"_\")\n",
        "#        \n",
        "#   \n",
        "#      #print(f'Company name:{company_name.strip()}')\n",
        "#      #print(f'Position:{position.strip()}')\n",
        "#      #print(f'Worktime:{worktime.strip()}')\n",
        "#print(jobs)\n",
        "#\n",
        "#page = 1\n",
        "#while page != 13:\n",
        "#      url = f\"https://www.kariyer.net/is-ilanlari/afyon-{page}?ct=3&cp={page}\"\n",
        "#      print(url)\n",
        "#      page = page + 1"
      ]
    }
  ]
}